=============to get the pod service statefulset deployment_name================
oc get pod <pod_name> 
oc get deployment <deployment_name> 
oc get endpoints <service_name>
oc get statefulset <statefulset_name>

===================to get the information about above objects=====
oc get deployment biz-sgb-service -n boi-investments-biz-prod to get the deployment information
oc describe deployment biz-sgb-service --- to get the yaml file

    Mounts:
      /config from ciamregsrvc (ro)
  Volumes:
   ciamregsrvc:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  ciamregsrvc
    Optional:    false
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      False   MinimumReplicasUnavailable
  Progressing    True    ReplicaSetUpdated
OldReplicaSets:  ciamregsrvc-v1-7d86cc6ddb (2/2 replicas created), ciamregsrvc-v1-68bf9d9ffb (3/3 replicas created)
NewReplicaSet:   ciamregsrvc-v1-6c6d5d8776 (3/3 replicas created)
Events:          <none>
[l301@IBM.LOCAL@drinstlrcp4i01 ~]$ oc get deployment ciamregsrvc-v1
NAME             READY   UP-TO-DATE   AVAILABLE   AGE
ciamregsrvc-v1   0/6     3            0           150d
[l301@IBM.LOCAL@drinstlrcp4i01 ~]$ ip r ;date
default via 10.174.8.1 dev enp1s0 proto static metric 100
10.174.8.0/22 dev enp1s0 proto kernel scope link src 10.174.8.161 metric 100
10.174.246.0/23 dev enp3s0 proto kernel scope link src 10.174.246.161 metric 101
Wed Nov 22 17:26:25 IST 2023


===========to check the pods events=================
oc get events --field-selector involvedObject.name=biz-nps-contribution-service-667ffdb7cd-2czgr

LAST SEEN   TYPE      REASON    OBJECT                                              MESSAGE
13m         Normal    Pulled    pod/biz-nps-contribution-service-667ffdb7cd-2czgr   Container image "dr-quay-quay-quay-enterprise.apps.dr.ibm.local/ibmprod/biz-nps-contribution-service:2f7efc8" already present on machine
143m        Normal    Created   pod/biz-nps-contribution-service-667ffdb7cd-2czgr   Created container biz-nps-contribution-service
3m42s       Warning   BackOff   pod/biz-nps-contribution-service-667ffdb7cd-2czgr   Back-off restarting failed container biz-nps-contribution-service in pod biz-nps-contribution-service-667ffdb7cd-2czgr_boi-money-movement-biz-prod(2f22f067-6ee8-41fc-8fdf-cc1a4a6df4f7)
[l301@IBM.LOCAL@drinstlrcp4i01 ~]$ oc get po
NAME                                            READY   STATUS             RESTARTS           AGE
biz-nps-contribution-service-667ffdb7cd-2czgr   1/2     CrashLoopBackOff   2151 (2m56s ago)   8d
biz-nps-contribution-service-667ffdb7cd-4w9zw   1/2     CrashLoopBackOff   2150 (2m10s ago)   8d
biz-nps-contribution-service-667ffdb7cd-dgfrn   1/2     CrashLoopBackOff   2153 (3m11s ago)   8d
============== To check the existing pod is replica for removed one====================

oc get pods  --show-labels |grep -i kieserver-dr
kieserver-dr-2-deploy                           0/1     Completed   0               5d20h   openshift.io/deployer-pod-for.name=kieserver-dr-2
kieserver-dr-2-xq7qq                            1/1     Running     1 (3d19h ago)   5d20h   app=rhpam-dr,application=rhpam-dr,cluster=jgrp.k8s.kieserver-dr,com.company=Red_Hat,deployment=kieserver-dr-2,deploymentConfig=kieserver-dr,deploymentconfig=kieserver-dr,rht.comp=PAM,rht.comp_ver=7.13.4,rht.prod_name=Red_Hat_Process_Automation,rht.prod_ver=7.13.4,rht.subcomp=rhpam-kieserver-rhel8,rht.subcomp_t=application,service=kieserver-dr,services.server.kie.org/kie-server-id=kieserver-prod


oc get po
NAME                                            READY   STATUS      RESTARTS        AGE
business-automation-operator-5f8bb4d995-6vpsb   1/1     Running     0               5d20h
console-cr-form                                 2/2     Running     0               8d
kieserver-dr-2-deploy                           0/1     Completed   0               5d20h
kieserver-dr-2-xq7qq                            1/1     Running     1 (3d19h ago)   5d20h
rhpam-dr-rhpamcentrmon-3-ph4r5                  1/1     Running     0               8d
rhpam-dr-rhpamcentrmon-4-deploy                 0/1     Error       0               5d20h
[l301@IBM.LOCAL@drinstlrcp4i01 ~]$

=================== How to check the jobs================================
[l301@IBM.LOCAL@drinstlrcp4i01 ~]$ oc get jobs
NAME                    COMPLETIONS   DURATION   AGE
image-pruner-27982080   0/1           251d       251d
image-pruner-28340640   1/1           9s         2d13h
image-pruner-28342080   1/1           17s        37h
image-pruner-28343520   1/1           19s        13h
[l301@IBM.LOCAL@drinstlrcp4i01 ~]$ oc get po |grep -i image-pruner
image-pruner-28340640-g49jn                        0/1     Completed   0          2d13h
image-pruner-28342080-dwls8                        0/1     Completed   0          37h
image-pruner-28343520-nx9hz                        0/1     Completed   0          13h

Read the deployemt config to understand the job 

[l301@IBM.LOCAL@drinstlrcp4i01 ~]$ oc describe cronjob image-pruner
Name:                          image-pruner
Namespace:                     openshift-image-registry
Labels:                        <none>
Annotations:                   imageregistry.operator.openshift.io/checksum: sha256:d0d5d91fe589a3c666c1340c59931371b1db9f3a65d8b6a50039095793f9dec4
Schedule:                      0 0 * * *
Concurrency Policy:            Forbid
Suspend:                       False
Successful Job History Limit:  3
Failed Job History Limit:      3
Starting Deadline Seconds:     3600s
Selector:                      <unset>
Parallelism:                   <unset>
Completions:                   <unset>
Pod Template:
  Labels:           <none>
  Service Account:  pruner
  Containers:
   image-pruner:
    Image:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:504f69082c90b3885d737888d3cea231f6577eb517424c80372745ca688a0379
    Port:       <none>
    Host Port:  <none>
    Command:
      /bin/sh

===========================Openshift-deamonset=======================================

first check the specific nodes are running on project 
[l301@IBM.LOCAL@prdintrlrrh01 ~]$ oc get po  |grep -i dns-default
dns-default-4s2cx     2/2     Running   0          82d
dns-default-8qzc4     2/2     Running   0          89d
dns-default-8vtqs     2/2     Running   0          90d
dns-default-d9b69     2/2     Running   0          84d
dns-default-gcn4p     2/2     Running   0          85d
dns-default-j7p6j     2/2     Running   4          199d
dns-default-jfqj4     2/2     Running   4          83d
dns-default-k9jlm     2/2     Running   0          87d
dns-default-m777t     2/2     Running   14         199d
dns-default-mq7h7     2/2     Running   0          86d
dns-default-nn6rq     2/2     Running   0          88d
dns-default-q8jng     2/2     Running   4          91d
dns-default-vhxnd     2/2     Running   4          199d
dns-default-w2kzv     2/2     Running   0          87d




then  check demonset config with below flgas to versify node selector where pods should be run

[l301@IBM.LOCAL@prdintrlrrh01 ~]$ oc get daemonset  dns-default -o yaml |grep -i kubernetes.io/os
        kubernetes.io/os: linux
[l301@IBM.LOCAL@prdintrlrrh01 ~]$ oc get daemonset  dns-default -o yaml |grep -i key
        - --tls-private-key-file=/etc/tls/private/tls.key
      - key: node-role.kubernetes.io/master
          - key: Corefile
[l301@IBM.LOCAL@prdintrlrrh01 ~]$



compair above node selector wih below values 


[l301@IBM.LOCAL@prdintrlrrh01 ~]$ oc  get pods -o wide -n openshift-dns
NAME                  READY   STATUS    RESTARTS   AGE    IP             NODE                         NOMINATED NODE   READINESS GATES
dns-default-4s2cx     2/2     Running   0          82d    10.128.8.4     infra-rh-01.prd.ibm.local    <none>           <none>
dns-default-8qzc4     2/2     Running   0          89d    10.131.2.2     worker-rh-03.prd.ibm.local   <none>           <none>
dns-default-8vtqs     2/2     Running   0          90d    10.130.2.2     worker-rh-02.prd.ibm.local   <none>           <none>
dns-default-d9b69     2/2     Running   0          84d    10.130.6.3     worker-rh-09.prd.ibm.local   <none>           <none>
dns-default-gcn4p     2/2     Running   0          85d    10.129.6.2     worker-rh-08.prd.ibm.local   <none>           <none>
dns-default-j7p6j     2/2     Running   4          199d   10.129.0.7     master-rh-02.prd.ibm.local   <none>           <none>
dns-default-jfqj4     2/2     Running   4          83d    10.131.6.81    worker-rh-10.prd.ibm.local   <none>           <none>
dns-default-k9jlm     2/2     Running   0          87d    10.130.4.4     worker-rh-05.prd.ibm.local   <none>           <none>
dns-default-m777t     2/2     Running   14         199d   10.128.0.3     master-rh-03.prd.ibm.local   <none>           <none>
dns-default-mq7h7     2/2     Running   0          86d    10.128.6.4     worker-rh-07.prd.ibm.local   <none>           <none>
dns-default-nn6rq     2/2     Running   0          88d    10.128.4.3     worker-rh-04.prd.ibm.local   <none>           <none>
dns-default-q8jng     2/2     Running   4          91d    10.128.2.9     worker-rh-01.prd.ibm.local   <none>           <none>
dns-default-vhxnd     2/2     Running   4          199d   10.130.0.7     master-rh-01.prd.ibm.local   <none>           <none>
dns-default-w2kzv     2/2     Running   0          87d    10.131.4.2     worker-rh-06.prd.ibm.local   <none>           <none>
node-resolver-7v52t   1/1     Running   2          91d    10.172.8.57    worker-rh-01.prd.ibm.local   <none>           <none>
node-resolver-bcsnr   1/1     Running   0          88d    10.172.8.60    worker-rh-04.prd.ibm.local   <none>           <none>
node-resolver-bwhcq   1/1     Running   0          84d    10.172.8.55    worker-rh-09.prd.ibm.local   <none>           <none>
node-resolver-bzff9   1/1     Running   2          199d   10.172.8.168   master-rh-01.prd.ibm.local   <none>           <none>
node-resolver-fnnm4   1/1     Running   0          87d    10.172.8.61    worker-rh-05.prd.ibm.local   <none>           <none>
node-resolver-h698r   1/1     Running   0          87d    10.172.8.62    worker-rh-06.prd.ibm.local   <none>           <none>
node-resolver-hbfnx   1/1     Running   0          86d    10.172.8.63    worker-rh-07.prd.ibm.local   <none>           <none>
node-resolver-hxrtt   1/1     Running   0          90d    10.172.8.58    worker-rh-02.prd.ibm.local   <none>           <none>
node-resolver-jbg24   1/1     Running   0          82d    10.172.8.51    infra-rh-01.prd.ibm.local    <none>           <none>
node-resolver-lqkjw   1/1     Running   3          199d   10.172.8.52    infra-rh-02.prd.ibm.local    <none>           <none>
node-resolver-mdjzz   1/1     Running   3          83d    10.172.8.56    worker-rh-10.prd.ibm.local   <none>           <none>
node-resolver-rjdfw   1/1     Running   2          199d   10.172.8.169   master-rh-02.prd.ibm.local   <none>           <none>
node-resolver-t6skj   1/1     Running   7          199d   10.172.8.170   master-rh-03.prd.ibm.local   <none>           <none>
node-resolver-vqcff   1/1     Running   0          85d    10.172.8.54    worker-rh-08.prd.ibm.local   <none>           <none>
node-resolver-z6rmh   1/1     Running   0          89d    10.172.8.59    worker-rh-03.prd.ibm.local   <none>           <none>
node-resolver-ztd48   1/1     Running   0          81d    10.172.8.53    infra-rh-03.prd.ibm.local    <none>           <none>




[l301@IBM.LOCAL@prdintrlrrh01 ~]$ oc get node infra-rh-01.prd.ibm.local --show-labels
NAME                        STATUS   ROLES          AGE   VERSION           LABELS
infra-rh-01.prd.ibm.local   Ready    infra,worker   82d   v1.25.8+27e744f   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=infra-rh-01.prd.ibm.local,kubernetes.io/os=linux,node-role.kubernetes.io/infra=,node-role.kubernetes.io/worker=,node-role.kubernetes.io=infra,node.openshift.io/os_id=rhcos
[l301@IBM.LOCAL@prdintrlrrh01 ~]$ oc get node worker-rh-03.prd.ibm.local --show-labels
NAME                         STATUS   ROLES    AGE   VERSION           LABELS
worker-rh-03.prd.ibm.local   Ready    worker   89d   v1.25.8+27e744f   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=worker-rh-03.prd.ibm.local,kubernetes.io/os=linux,node-role.kubernetes.io/worker=,node.openshift.io/os_id=rhcos
[l301@IBM.LOCAL@prdintrlrrh01 ~]$ oc get node worker-rh-02.prd.ibm.local --show-labels
NAME                         STATUS   ROLES    AGE   VERSION           LABELS
worker-rh-02.prd.ibm.local   Ready    worker   90d   v1.25.8+27e744f   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=worker-rh-02.prd.ibm.local,kubernetes.io/os=linux,node-role.kubernetes.io/worker=,node.openshift.io/os_id=rhcos
[l301@IBM.LOCAL@prdintrlrrh01 ~]$ oc get node worker-rh-09.prd.ibm.local --show-labels
NAME                         STATUS   ROLES    AGE   VERSION           LABELS
worker-rh-09.prd.ibm.local   Ready    worker   84d   v1.25.8+27e744f   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=worker-rh-09.prd.ibm.local,kubernetes.io/os=linux,node-role.kubernetes.io/worker=,node.openshift.io/os_id=rhcos
[l301@IBM.LOCAL@prdintrlrrh01 ~]$ oc get node worker-rh-08.prd.ibm.local master-rh-02.prd.ibm.local worker-rh-10.prd.ibm.local worker-rh-05.prd.ibm.local master-rh-03.prd.ibm.local worker-rh-07.prd.ibm.local worker-rh-04.prd.ibm.local worker-rh-01.prd.ibm.local master-rh-01.prd.ibm.local worker-rh-06.prd.ibm.local  --show-labels
NAME                         STATUS   ROLES    AGE    VERSION           LABELS
worker-rh-08.prd.ibm.local   Ready    worker   85d    v1.25.8+27e744f   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=worker-rh-08.prd.ibm.local,kubernetes.io/os=linux,node-role.kubernetes.io/worker=,node.openshift.io/os_id=rhcos
master-rh-02.prd.ibm.local   Ready    master   357d   v1.25.8+27e744f   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=master-rh-02.prd.ibm.local,kubernetes.io/os=linux,node-role.kubernetes.io/master=,node.openshift.io/os_id=rhcos
worker-rh-10.prd.ibm.local   Ready    worker   83d    v1.25.8+27e744f   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=worker-rh-10.prd.ibm.local,kubernetes.io/os=linux,node-role.kubernetes.io/worker=,node.openshift.io/os_id=rhcos
worker-rh-05.prd.ibm.local   Ready    worker   87d    v1.25.8+27e744f   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=worker-rh-05.prd.ibm.local,kubernetes.io/os=linux,node-role.kubernetes.io/worker=,node.openshift.io/os_id=rhcos
master-rh-03.prd.ibm.local   Ready    master   357d   v1.25.8+27e744f   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=master-rh-03.prd.ibm.local,kubernetes.io/os=linux,node-role.kubernetes.io/master=,node.openshift.io/os_id=rhcos
worker-rh-07.prd.ibm.local   Ready    worker   86d    v1.25.8+27e744f   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=worker-rh-07.prd.ibm.local,kubernetes.io/os=linux,node-role.kubernetes.io/worker=,node.openshift.io/os_id=rhcos
worker-rh-04.prd.ibm.local   Ready    worker   88d    v1.25.8+27e744f   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=worker-rh-04.prd.ibm.local,kubernetes.io/os=linux,node-role.kubernetes.io/worker=,node.openshift.io/os_id=rhcos
worker-rh-01.prd.ibm.local   Ready    worker   91d    v1.25.8+27e744f   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=worker-rh-01.prd.ibm.local,kubernetes.io/os=linux,node-role.kubernetes.io/worker=,node.openshift.io/os_id=rhcos
master-rh-01.prd.ibm.local   Ready    master   357d   v1.25.8+27e744f   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=master-rh-01.prd.ibm.local,kubernetes.io/os=linux,node-role.kubernetes.io/master=,node.openshift.io/os_id=rhcos
worker-rh-06.prd.ibm.local   Ready    worker   87d    v1.25.8+27e744f   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=worker-rh-06.prd.ibm.local,kubernetes.io/os=linux,node-role.kubernetes.io/worker=,node.openshift.io/os_id=rhcos


===================Cluster operator failure====================================
check csv running on which project 
oc get svc --all-namespaces  |grep -i insights
openshift-insights                                 metrics                                                           ClusterIP      172.30.150.117   <none>                                                                              443/TCP                                                                                                    362d

and enter into project and verify the repective pod status 
 oc get project |grep -i insights
openshift-insights                                                              Active

verify the operator configuration
oc describe clusteroperator insights

and  verify the eroor code on repective pod logs
oc logs pod/insights-operator-6bf6b7967d-tvql7 |grep -i UploadFailed
I1119 16:16:45.392872       1 controllerstatus.go:79] name=insightsuploader healthy=false reason=UploadFailed message=Unable to report: unable to build request to connect to Insights server: Post "https://console.redhat.com/api/ingress/v1/upload": net/http: TLS handshake timeout
I1119 16:36:43.506522       1 controllerstatus.go:79] name=insightsuploader healthy=false reason=UploadFailed message=Unable to report: unable to build request to connect to Insights server: Post "https://console.redhat.com/api/ingress/v1/upload": x509: certificate signed by unknown authority

[l301@IBM.LOCAL@prdintrlrrh01 ~]$ oc get project |grep -i openshift-cluster-version
openshift-cluster-version                                                                    Active
[l301@IBM.LOCAL@prdintrlrrh01 ~]$ oc project openshift-cluster-version
Already on project "openshift-cluster-version" on server "https://api.prd.ibm.local:6443".
[l301@IBM.LOCAL@prdintrlrrh01 ~]$ oc get svc
NAME                       TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
cluster-version-operator   ClusterIP   172.30.110.197   <none>        9099/TCP   361d
[l301@IBM.LOCAL@prdintrlrrh01 ~]$

Console", "Insights", "Storage", "baremetal", "marketplace", "openshift-samples"}}, ImplicitlyEnabledCaps:[]v1.ClusterVersionCapability(nil)}}
I1127 01:31:18.768491       1 status.go:90] merge into existing history completed=true desired=v1.Release{Version:"4.12.14", Image:"quay.io/openshift-release-dev/ocp-release@sha256:157cc02d63bfe67988429fd803da632e495e230d811759b1aed1e6ffa7a3f31a", URL:"https://access.redhat.com/errata/RHBA-2023:1858", Channels:[]string{"candidate-4.12", "candidate-4.13", "eus-4.12", "fast-4.12", "fast-4.13", "stable-4.12", "stable-4.13"}} last=&v1.UpdateHistory{State:"Completed", StartedTime:time.Date(2023, time.May, 7, 13, 43, 29, 0, time.Local), CompletionTime:time.Date(2023, time.May, 7, 14, 56, 56, 0, time.Local), Version:"4.12.14", Image:"quay.io/openshift-release-dev/ocp-release@sha256:157cc02d63bfe67988429fd803da632e495e230d811759b1aed1e6ffa7a3f31a", Verified:true, AcceptedRisks:""}
I1127 01:31:18.786422       1 cvo.go:572] Finished syncing cluster version "openshift-cluster-version/version" (18.720103ms)
[l301@IBM.LOCAL@prdintrlrrh01 ~]$ oc get clusterversion
NAME      VERSION   AVAILABLE   PROGRESSING   SINCE   STATUS
version   4.12.14   True        False         203d    Cluster version is 4.12.14
[l301@IBM.LOCAL@prdintrlrrh01 ~]$ ip r l;date
default via 10.172.8.1 dev enp1s0 proto static metric 100
10.172.8.0/22 dev enp1s0 proto kernel scope link src 10.172.8.166 metric 100
10.172.246.0/23 dev enp3s0 proto kernel scope link src 10.172.246.166 metric 101
Mon Nov 27 07:02:39 IST 2023

==========================compliancescan =================================

[l301@IBM.LOCAL@prdintrlrrh01 ~]$ oc get project  |grep -i complian
openshift-compliance                                                                         Active
policy-compliance                                                                            Active
[l301@IBM.LOCAL@prdintrlrrh01 ~]$ oc get compliancescan |grep -i openshift-compliance
No resources found in openshift-image-registry namespace.
[l301@IBM.LOCAL@prdintrlrrh01 ~]$ oc get compliancescan -n openshift-compliance
NAME                   PHASE   RESULT
ocp4-cis               DONE    NON-COMPLIANT
ocp4-cis-node-master   DONE    NON-COMPLIANT
ocp4-cis-node-worker   DONE    NON-COMPLIANT
[l301@IBM.LOCAL@prdintrlrrh01 ~]$ oc get compliancescan -n policy-compliance
No resources found in policy-compliance namespace.
[l301@IBM.LOCAL@prdintrlrrh01 ~]$ ip r l;date
default via 10.172.8.1 dev enp1s0 proto static metric 100
10.172.8.0/22 dev enp1s0 proto kernel scope link src 10.172.8.166 metric 100
10.172.246.0/23 dev enp3s0 proto kernel scope link src 10.172.246.166 metric 101
Mon Nov 27 20:01:46 IST 2023
[l301@IBM.LOCAL@prdintrlrrh01 ~]$


======================PVC===============================
oc get pv  |grep -i prodamqbroker-prodamqbroker-ss-0
pvc-809fd0d7-9a4e-4ca0-ae72-4d0b652ce90c   990Gi      RWO            Retain           Bound      boiprod-amq/prodamqbroker-prodamqbroker-ss-0                                         ocs-external-storagecluster-ceph-rbd            154d

oc project boiprod-amq


oc get po |grep -i prodamqbroker-prodamqbroker-ss-0
prodamqbroker-ss-0   1/1     Running   1 (5d7h ago)   89d
prodamqbroker-ss-1   1/1     Running   0              89d

oc get pod prodamqbroker-ss-0 -n boiprod-amq -o yaml 

  - name: prodamqbroker
      claimName: prodamqbroker-prodamqbroker-ss-0
      name: prodamqbroker-props-00000001
    name: configmap-prodamqbroker-props-00000001
    name: amq-cfg-dir
    name: tool-dir
  - name: kube-api-access-6blnm
          name: kube-root-ca.crt
              fieldPath: metadata.namespace
            path: namespace
          name: openshift-service-ca.crt
    name: prodamqbroker-container
    name: prodamqbroker-container-init
	
	
	 oc  describe pvc image-registry-pvc

Name:          image-registry-pvc
Namespace:     openshift-image-registry
StorageClass:  ocs-external-storagecluster-cephfs
Status:        Bound
Volume:        pvc-1bdba63c-cfcf-4ba0-9f34-dbc67bd947f2
Labels:        <none>
Annotations:   pv.kubernetes.io/bind-completed: yes
               pv.kubernetes.io/bound-by-controller: yes
               volume.beta.kubernetes.io/storage-provisioner: openshift-storage.cephfs.csi.ceph.com
               volume.kubernetes.io/storage-provisioner: openshift-storage.cephfs.csi.ceph.com
Finalizers:    [kubernetes.io/pvc-protection]
Capacity:      200Gi
Access Modes:  RWX
VolumeMode:    Filesystem
Used By:       image-registry-7cbd4b98cc-4q8l4
               image-registry-7cbd4b98cc-c5zsq
               image-registry-7cbd4b98cc-x69wt
Events:        <none>


======================cluster updates===============================

oc get clusterversion

NAME      VERSION   AVAILABLE   PROGRESSING   SINCE   STATUS
version   4.12.15   True        False         199d    Error while reconciling 4.12.15: the cluster operator insights is not availabl

[l301@IBM.LOCAL@prdintrlrrh01 ~]$ oc get clusteroperator insights
NAME       VERSION   AVAILABLE   PROGRESSING   DEGRADED   SINCE   MESSAGE
insights   4.12.15   False       False         True       35d     Unable to report: unable to build request to connect to Insights server: Post "https://console.redhat.com/api/ingress/v1/upload": x509: certificate signed by unknown authority
[l301@IBM.LOCAL@prdintrlrrh01 ~]$ ip r l;date
default via 10.172.8.1 dev enp1s0 proto static metric 100
10.172.8.0/22 dev enp1s0 proto kernel scope link src 10.172.8.166 metric 100
10.172.246.0/23 dev enp3s0 proto kernel scope link src 10.172.246.166 metric 101
Thu Nov 23 08:57:00 IST 2023

[l301@IBM.LOCAL@prdintrlrrh01 ~]$     oc get clusteroperators |grep -i version
NAME                                       VERSION   AVAILABLE   PROGRESSING   DEGRADED   SINCE   MESSAGE
kube-storage-version-migrator              4.12.14   True        False         False      165d
[l301@IBM.LOCAL@prdintrlrrh01 ~]$




oc adm upgrade info
Cluster version is 4.12.14

 

Upgradeable=False

 

  Reason: MultipleReasons
  Message: Cluster should not be upgraded between minor versions for multiple reasons: AdminAckRequired,ClusterOperatorsNotUpgradeable
  * Kubernetes 1.26 and therefore OpenShift 4.13 remove several APIs which require admin consideration. Please see the knowledge article https://access.redhat.com/articles/6958394 for details and instructions.
  * Multiple cluster operators should not be upgraded between minor versions:
  * Cluster operator cloud-controller-manager should not be upgraded between minor versions: AsExpected:
  * Cluster operator operator-lifecycle-manager should not be upgraded between minor versions: IncompatibleOperatorsInstalled: ClusterServiceVersions blocking cluster upgrade: openshift-storage/odf-operator.v4.11.11 is incompatible with OpenShift minor versions greater than 4.12

 for more info  https://access.redhat.com/solutions/5636241
========================Promotious monitoring not sending mails=================
enter into project 
oc project openshift-monitoring

check the monitoring pod logs 
oc get po
prometheus-k8s-0
oc logs pod/prometheus-k8s-0 |grep -i send

ts=2023-09-01T20:58:18.172Z caller=notifier.go:534 level=error component=notifier alertmanager=https://alertmanager-open-cluster-management-observability.apps.prd.ibm.local/api/v2/alerts count=1 msg="Error sending alert" err="Post \"https://alertmanager-open-cluster-management-observability.apps.prd.ibm.local/api/v2/alerts\": read tcp 10.129.9.152:56102->10.172.8.233:443: read: connection reset by peer"

enter into pod 
oc rsh prometheus-k8s-0

check the communication

 curl -v https://alertmanager-open-cluster-management-observability.apps.prd.ibm.local/api/v2/alerts
*   Trying 10.172.8.233...
* TCP_NODELAY set
* Connected to alertmanager-open-cluster-management-observability.apps.prd.ibm.local (10.172.8.233) port 443 (#0)
* ALPN, offering h2
* ALPN, offering http/1.1
* successfully set certificate verify locations:
*   CAfile: /etc/pki/tls/certs/ca-bundle.crt
  CApath: none
* TLSv1.3 (OUT), TLS handshake, Client hello (1):
* TLSv1.3 (IN), TLS handshake, Server hello (2):
* TLSv1.3 (IN), TLS handshake, [no content] (0):
* TLSv1.3 (IN), TLS handshake, Encrypted Extensions (8):
* TLSv1.3 (IN), TLS handshake, [no content] (0):
* TLSv1.3 (IN), TLS handshake, Certificate (11):
* TLSv1.3 (OUT), TLS alert, unknown CA (560):
* SSL certificate problem: self signed certificate in certificate chain
* Closing connection 0
curl: (60) SSL certificate problem: self signed certificate in certificate chain
More details here: https://curl.haxx.se/docs/sslcerts.html

curl failed to verify the legitimacy of the server and therefore could not
establish a secure connection to it. To learn more about this situation and
how to fix it, please visit the web page mentioned above.

===================Ceph======================
ceph orch ps  |grep -i mon.phmumrhnprdodf01
mon.phmumrhnprdodf01                             phmumrhnprdodf01               running (6M)    12s ago  12M    1128M    2048M  16.2.8-85.el8cp  b2c997ff1898  d820ef38faf3


ceph -s

  cluster:
    id:     30ab7650-5b59-11ed-b0ea-566fde920028
    health: HEALTH_OK

  services:
    mon: 5 daemons, quorum nprdcephadmin01,phmumrhnprdodf01,phmumrhnprdodf02,phmumrhnprdodf03,nprdcephdash01 (age 8d)
    mgr: phmumrhnprdodf02.lxldya(active, since 6w), standbys: nprdcephadmin01.flzbni, phmumrhnprdodf03.zzlsgc, phmumrhnprdodf01.qpjtxu, nprdcephdash01.gorhgr
    mds: 2/2 daemons up, 4 standby
    osd: 21 osds: 21 up (since 8d), 21 in (since 6M)
    rgw: 6 daemons active (3 hosts, 1 zones)

  data:
    volumes: 2/2 healthy
    pools:   29 pools, 1089 pgs
    objects: 20.60M objects, 9.6 TiB
    usage:   29 TiB used, 44 TiB / 73 TiB avail
    pgs:     1089 active+clean

  io:
    client:   21 KiB/s rd, 5.6 MiB/s wr, 4 op/s rd, 346 op/s wr



ceph osd status



===================fluentd log collector configmaps=================

oc project openshift-logging


[l301@IBM.LOCAL@prdintrlrrh01 ~]$ oc get po |grep -i collector-dkgpj
collector-dkgpj                                 1/1     Running     0          12d
[l301@IBM.LOCAL@prdintrlrrh01 ~]$ oc logs pod/collector-dkgpj  |grep -i slow_flush_log_threshold
2023-11-10 08:47:18 +0000 [warn]: [fluentd_logforword] buffer flush took longer time than slow_flush_log_threshold: elapsed_time=26.713233618065715 slow_flush_log_threshold=20.0 plugin_id="fluentd_logforword"
2023-11-13 12:57:07 +0000 [warn]: [fluentd_logforword] buffer flush took longer time than slow_flush_log_threshold: elapsed_time=42.38737848959863 slow_flush_log_threshold=20.0 plugin_id="fluentd_logforword"
2023-11-15 06:49:46 +0000 [warn]: [fluentd_logforword] buffer flush took longer time than slow_flush_log_threshold: elapsed_time=28.851968499831855 slow_flush_log_threshold=20.0 plugin_id="fluentd_logforword"
2023-11-18 18:22:52 +0000 [warn]: [fluentd_logforword] buffer flush took longer time than slow_flush_log_threshold: elapsed_time=21.737713905982673 slow_flush_log_threshold=20.0 plugin_id="fluentd_logforword"
2023-11-18 18:23:53 +0000 [warn]: [fluentd_logforword] buffer flush took longer time than slow_flush_log_threshold: elapsed_time=59.64234469458461 slow_flush_log_threshold=20.0 plugin_id="fluentd_logforword"



==================crash loopbackoff================
connect to project 

check the pod status 

 oc get po |grep -i ibm-apiconnect
 ibm-apiconnect-756697c7c7-9mx5n                          0/1     CrashLoopBackOff   38810 (2m8s ago)   182d


================to check the resouces utilization for nodes (SystemMemoryExceedsReservation)=====================
login to cluster 

oc describe node worker-rh-03.nprd.ibm.local

Addresses:
  InternalIP:  10.173.8.66
  Hostname:    worker-rh-03.nprd.ibm.local
Capacity:
  cpu:                96
  ephemeral-storage:  936629228Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             527662252Ki
  pods:               250
Allocatable:
  cpu:                95500m
  ephemeral-storage:  910079170460
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             526408876Ki
  pods:               250
System Info:
  Machine ID:                                a96f0363d9dc4299a9dde093906cd010
  System UUID:                               4c4c4544-0053-3910-8051-c4c04f465233
  Boot ID:                                   1d090541-d6ba-4e49-99cc-c0ea25566073
  Kernel Version:                            4.18.0-372.51.1.el8_6.x86_64
  OS Image:                                  Red Hat Enterprise Linux CoreOS 412.86.202304131008-0 (Ootpa)
  Operating System:                          linux
  Architecture:                              amd64
  Container Runtime Version:                 cri-o://1.25.3-2.rhaos4.12.git592efcd.el8
  Kubelet Version:                           v1.25.8+27e744f
  Kube-Proxy Version:                        v1.25.8+27e744f

 

Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests           Limits
  --------           --------           ------
  cpu                60242m (63%)       200070m (209%)
  memory             158987311Ki (30%)  272027246208 (50%)
  ephemeral-storage  0 (0%)             0 (0%)
  hugepages-1Gi      0 (0%)             0 (0%)
  hugepages-2Mi      0 (0%)             0 (0%)
Events:              <none>

for depth analsysis we can login into worker node and check the resouces utilization

 oc debug node/[node_name]


==================to check the pod resouces utilization==================

oc top pod collector-xbtgc -n <namespace>

=====================Csv verification========================
[l301@IBM.LOCAL@prdintrlrrh01 ~]$ oc get pods --all-namespaces |grep -i olm-operator
openshift-operator-lifecycle-manager               olm-operator-649d75bb58-rtrzw                                     1/1     Running

oc project openshift-operator-lifecycle-manager

oc logs pod/olm-operator-649d75bb58-rtrzw

time="2023-11-23T00:59:00Z" level=info msg="install strategy successful" csv=advanced-cluster-management.v2.6.8 id=XRBFh namespace=open-cluster-management phase=Installing strategy=deployment
time="2023-11-23T00:59:00Z" level=info msg="install strategy successful" csv=advanced-cluster-management.v2.6.8 id=bZO2Z namespace=open-cluster-management phase=Installing strategy=deployment



[l301@IBM.LOCAL@prdintrlrrh01 ~]$ oc get csv --all-namespaces|grep -i advanced-cluster-management
open-cluster-management                            advanced-cluster-management.v2.6.8             Advanced Cluster Management for Kubernetes   2.6.8                   advanced-cluster-management.v2.6.7             Succeeded

=======================There are errors when sending requests to remote write endpoint: thanos-receiver(ACMRemoteWriteError)============

oc get pods --all-namespaces |grep -i observability-observatorium
open-cluster-management-observability              observability-observatorium-api-86dfc7cd68-dpl2g                  1/1     Running     0                 28d
open-cluster-management-observability              observability-observatorium-api-86dfc7cd68-jndmh                  1/1     Running     0                 28d
open-cluster-management-observability              observability-observatorium-operator-6f6dd7577f-vwsj9             1/1     Running     0                 57d

oc project open-cluster-management-observability


oc get po |grep -i observability-observatorium-api
observability-observatorium-api-86dfc7cd68-dpl2g           1/1     Running   0             28d
observability-observatorium-api-86dfc7cd68-jndmh           1/1     Running   0             28d

oc logs pod/observability-observatorium-api-86dfc7cd68-dpl2g

level=error name=observatorium ts=2023-11-15T02:35:43.337222765Z caller=logchannel.go:133 msg="failed to forward metrics" returncode="409 Conflict" response="3 errors: replicate write request for endpoint observability-thanos-receive-default-2.observability-thanos-receive-default.open-cluster-management-observability.svc.cluster.local:10901: quorum not reached: conflict; replicate write request for endpoint observability-thanos-receive-default-0.observability-thanos-receive-default.open-cluster-management-observability.svc.cluster.local:10901: quorum not reached: conflict; replicate write request for endpoint observability-thanos-receive-default-1.observability-thanos-receive-default.open-cluster-management-observability.svc.cluster.local:10901: quorum not reached: conflict\n" url=http://observability-thanos-receive.open-cluster-management-observability.svc.cluster.local:19291/api/v1/receive

oc get po |grep -i observability-thanos-receive-default
observability-thanos-receive-default-0                     1/1     Running   0             88d
observability-thanos-receive-default-1                     1/1     Running   0             85d
observability-thanos-receive-default-2                     1/1     Running   0             77d


============================servicemonitor===========================
NAME                       AGE
prometheus-operator        306d
prometheus-user-workload   306d
thanos-ruler               306d
thanos-sidecar             306d

[l301@IBM.LOCAL@drinstlrcp4i01 ~]$ oc describe servicemonitor prometheus-operator |tail -20
    Bearer Token Secret:
      Key:
    Honor Labels:  true
    Port:          https
    Scheme:        https
    Tls Config:
      Ca:
      Ca File:  /etc/prometheus/configmaps/serving-certs-ca-bundle/service-ca.crt
      Cert:
      Cert File:    /etc/prometheus/secrets/metrics-client-certs/tls.crt
      Key File:     /etc/prometheus/secrets/metrics-client-certs/tls.key
      Server Name:  prometheus-operator.openshift-user-workload-monitoring.svc
  Namespace Selector:
  Selector:
    Match Labels:
      app.kubernetes.io/component:  controller
      app.kubernetes.io/name:       prometheus-operator
      app.kubernetes.io/part-of:    openshift-monitoring
      app.kubernetes.io/version:    0.63.0
Events:                             <none>
[l301@IBM.LOCAL@drinstlrcp4i01 ~]$


 oc logs svc/prometheus-operator
 
 nitoring prometheus=user-workload
level=info ts=2023-11-24T17:33:03.037429808Z caller=operator.go:1162 component=prometheusoperator key=openshift-user-workload-monitoring/user-workload msg="sync prometheus"
level=info ts=2023-11-24T17:33:03.037521639Z caller=operator.go:1330 component=prometheusoperator key=openshift-user-workload-monitoring/user-workload msg="update prometheus status"
level=warn ts=2023-11-24T17:33:03.07567775Z caller=operator.go:1917 component=prometheusoperator msg="skipping servicemonitor" error="it accesses file system via bearer token file which Prometheus specification prohibits" servicemonitor=openshift-operators/openshift-gitops-operator-metrics-monitor namespace=openshift-user-workload-monitoring prometheus=user-workload
level=info ts=2023-11-24T17:33:03.269265043Z caller=operator.go:1330 component=prometheusoperator key=openshift-user-workload-monitoring/user-workload msg="update prometheus status"



==============================AlertmanagerClusterFailedToSendAlerts(SMTP)========================
oc logs pod/alertmanager-main-1

celed after 4 attempts: establish connection to server: dial tcp 10.172.30.142:25: i/o timeout"
ts=2023-11-23T10:14:33.114Z caller=dispatch.go:354 level=error component=dispatcher msg="Notify for alerts failed" num_alerts=1 err="Default/email[0]: notify retry canceled after 4 attempts: establish connection to server: dial tcp 10.172.30.142:25: i/o timeout"
ts=2023-11-23T10:14:54.941Z caller=dispatch.go:354 level=error component=dispatcher msg="Notify for alerts failed" num_alerts=5 err="Default/email[0]: notify retry canceled after 4 attempts: establish connection to server: dial tcp 10.172.30.142:25: i/o timeout"
ts=2023-11-23T10:15:39.104Z caller=notify.go:732 level=warn component=dispatcher receiver=Default

[l301@IBM.LOCAL@prdintrlrrh01 ~]$ oc get pod alertmanager-main-1  -o wide
NAME                  READY   STATUS    RESTARTS      AGE   IP            NODE                              NOMINATED NODE   READINESS GATES
alertmanager-main-1   6/6     Running   1 (95d ago)   95d   10.131.0.89   worker-rh-01.prd-cp4i.ibm.local   <none>           <none>


=================================Fluentd cannot be scraped=========================
oc get pods --all-namespaces |grep -i collector

oc project projectname

oc get po -o wide |grep -i 10.129.9.176

oc logs pod/collector-ggdkn
=======================An Insights recommendation is active for this cluster.(how to search a POD with IP========================================

 oc get pods -o wide --all-namespaces |grep -i 10.129.0.31
 
 openshift-insights                                 insights-operator-565c5796cf-sp9gp                                1/1     Running                           1 (40d ago)         199d    10.129.0.31    master-rh-02.nprd.ibm.local   <none>           <none>

 oc project openshift-insights



